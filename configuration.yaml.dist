urls:
  -
    url: https://example.org
    # required to limit crawling to this domain
    allowedDomains:
      - example.org

crawler:
  # defaults to 0, thus only crawls the given urls and does not crawl any deeper
  maxDepth: 3
  # see https://github.com/yujiosaka/headless-chrome-crawler/blob/master/docs/API.md#crawlerqueueoptions
  options:
    userAgent: Custom Crawler Engine

tests:
  external_requests:
    allowed_domains:
      - www.example.net
  cookies:
    allowed:
      - _gat
  google_analytics:
    validIds:
      - 'UA-12345678-9'
  google_tagmanager:
    validIds:
      - 'GTM-12345'
  javascript_errors: ~
  seo: ~
  failed_requests: ~
