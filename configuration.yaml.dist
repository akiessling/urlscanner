urls:
  -
    url: https://example.org
    # allowedDomains is automatically set to the domain of the url, but you can add additional ones
    allowedDomains:
      - example.net

crawler:
  # defaults to 0, thus only crawls the given urls and does not crawl any deeper
  maxDepth: 3
  # see https://github.com/yujiosaka/headless-chrome-crawler/blob/master/docs/API.md#crawlerqueueoptions
  options:
    userAgent: Custom Crawler Engine

tests:
  external_requests:
    allowed_domains:
      - www.example.net
  cookies:
    allowed:
      - _gat
  google_analytics:
    validIds:
      - 'UA-12345678-9'
  google_tagmanager:
    validIds:
      - 'GTM-12345'
  javascript_errors: ~
  seo: ~
  failed_requests: ~
